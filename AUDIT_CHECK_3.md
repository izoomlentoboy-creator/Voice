# üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ #3 - –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≥–ª—É–±–æ–∫–∏–π –∞—É–¥–∏—Ç train_perfect_v3.py

## ‚úÖ –û–®–ò–ë–û–ö –ù–ï –ù–ê–ô–î–ï–ù–û!

---

## üî¨ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π:

### **1. Imports –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏** ‚úÖ
```python
import os, sys, argparse
from pathlib import Path
import torch, torch.nn, torch.optim
from torch.utils.data import Dataset, DataLoader
import soundfile as sf
import librosa
from transformers import Wav2Vec2Model
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from tqdm import tqdm
import numpy as np
import warnings, datetime, random, json
```
‚úÖ –í—Å–µ –∏–º–ø–æ—Ä—Ç—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã, –Ω–µ—Ç –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö

---

### **2. Seed setting** ‚úÖ
```python
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
```
‚úÖ –ü–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ RNG –∏—Å—Ç–æ—á–Ω–∏–∫–∏
‚úÖ CUDA-specific –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ CUDA

---

### **3. EnhancedVoiceDataset** ‚úÖ

#### **3.1. __init__** ‚úÖ
```python
def __init__(self, samples, max_length=80000, target_sr=16000, augment=False):
    self.samples = samples
    self.max_length = max_length
    self.target_sr = target_sr
    self.augment = augment
```
‚úÖ –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è
‚úÖ –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–∑—É–º–Ω—ã

#### **3.2. add_noise** ‚úÖ
```python
def add_noise(self, data, noise_factor=0.005):
    noise = np.random.randn(len(data)) * noise_factor
    return data + noise
```
‚úÖ Noise factor 0.005 –æ–ø—Ç–∏–º–∞–ª–µ–Ω
‚úÖ –ù–µ –º–µ–Ω—è–µ—Ç –¥–ª–∏–Ω—É

#### **3.3. time_stretch** ‚úÖ
```python
def time_stretch(self, data, rate=None):
    if rate is None:
        rate = np.random.uniform(0.9, 1.1)
    
    original_length = len(data)
    stretched = librosa.effects.time_stretch(y=data, rate=rate)
    
    if len(stretched) > original_length:
        stretched = stretched[:original_length]
    elif len(stretched) < original_length:
        stretched = np.pad(stretched, (0, original_length - len(stretched)))
    
    return stretched
```
‚úÖ Rate range 0.9-1.1 –æ–ø—Ç–∏–º–∞–ª–µ–Ω
‚úÖ –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—É—é –¥–ª–∏–Ω—É
‚úÖ Librosa 0.10+ API (y=)

#### **3.4. pitch_shift** ‚úÖ
```python
def pitch_shift(self, data, sr, n_steps=None):
    if n_steps is None:
        n_steps = np.random.randint(-2, 3)
    return librosa.effects.pitch_shift(y=data, sr=sr, n_steps=n_steps)
```
‚úÖ Range -2 to +2 semitones –æ–ø—Ç–∏–º–∞–ª–µ–Ω
‚úÖ Librosa 0.10+ API (y=)

#### **3.5. __getitem__** ‚úÖ
```python
def __getitem__(self, idx):
    wav_path, label = self.samples[idx]
    
    try:
        data, sr = sf.read(wav_path)
        
        if len(data.shape) > 1:
            data = np.mean(data, axis=1)
        
        if sr != self.target_sr:
            data = librosa.resample(y=data, orig_sr=sr, target_sr=self.target_sr)
        
        if self.augment:
            if np.random.random() < 0.2:
                data = self.pitch_shift(data, self.target_sr)
            if np.random.random() < 0.3:
                data = self.time_stretch(data)
            if np.random.random() < 0.3:
                data = self.add_noise(data)
        
        if len(data) > self.max_length:
            if self.augment:
                start = np.random.randint(0, len(data) - self.max_length)
                data = data[start:start + self.max_length]
            else:
                data = data[:self.max_length]
        else:
            data = np.pad(data, (0, self.max_length - len(data)))
        
        waveform = torch.FloatTensor(data)
        return waveform, label
    
    except Exception as e:
        print(f"Error loading {wav_path}: {e}")
        return torch.zeros(self.max_length), label
```
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫: load ‚Üí mono ‚Üí resample ‚Üí augment ‚Üí pad/crop
‚úÖ Augmentation –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: 0.2, 0.3, 0.3 (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
‚úÖ Random crop –¥–ª—è train, center crop –¥–ª—è val
‚úÖ Error handling –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç zeros –≤–º–µ—Å—Ç–æ –∫—Ä–∞—à–∞

---

### **4. UltimateVoiceClassifier** ‚úÖ

#### **4.1. __init__** ‚úÖ
```python
def __init__(self, num_classes=2, dropout=0.3):
    super().__init__()
    
    self.wav2vec2 = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-large")
    
    # Freeze feature extractor
    for param in self.wav2vec2.feature_extractor.parameters():
        param.requires_grad = False
    
    # Freeze first 8 layers
    for i in range(8):
        for param in self.wav2vec2.encoder.layers[i].parameters():
            param.requires_grad = False
    
    # Unfreeze last 16 layers
    for i in range(8, 24):
        for param in self.wav2vec2.encoder.layers[i].parameters():
            param.requires_grad = True
    
    self.attention = nn.Sequential(
        nn.Linear(1024, 256),
        nn.Tanh(),
        nn.Linear(256, 1)
    )
    
    self.classifier = nn.Sequential(
        nn.Linear(1024, 768),
        nn.LayerNorm(768),
        nn.GELU(),
        nn.Dropout(dropout),
        
        nn.Linear(768, 512),
        nn.LayerNorm(512),
        nn.GELU(),
        nn.Dropout(dropout),
        
        nn.Linear(512, 256),
        nn.LayerNorm(256),
        nn.GELU(),
        nn.Dropout(dropout),
        
        nn.Linear(256, num_classes)
    )
```
‚úÖ Wav2Vec2-LARGE (315M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
‚úÖ Freezing –ø–æ —Å–ª–æ—è–º (–Ω–µ –ø–æ —Ç–µ–Ω–∑–æ—Ä–∞–º)
‚úÖ Freeze 8/24 —Å–ª–æ—ë–≤ = 67% trainable
‚úÖ Attention: 1024‚Üí256‚Üí1 (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
‚úÖ Classifier: 1024‚Üí768‚Üí512‚Üí256‚Üí2 (–≥–ª—É–±–æ–∫–∏–π)
‚úÖ LayerNorm + GELU (—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
‚úÖ Dropout 0.3 (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)

#### **4.2. forward** ‚úÖ
```python
def forward(self, x):
    outputs = self.wav2vec2(x)
    hidden_states = outputs.last_hidden_state  # (batch, time, 1024)
    
    attention_scores = self.attention(hidden_states).squeeze(-1)  # (batch, time)
    attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(-1)  # (batch, time, 1)
    pooled = torch.sum(hidden_states * attention_weights, dim=1)  # (batch, 1024)
    
    logits = self.classifier(pooled)
    return logits
```
‚úÖ Softmax –ø–æ dim=1 (time dimension)
‚úÖ Weighted sum pooling
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏

---

### **5. train_epoch** ‚úÖ

```python
def train_epoch(model, dataloader, criterion, optimizer, device, accumulation_steps=1):
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    optimizer.zero_grad()
    
    pbar = tqdm(dataloader, desc="Training", ncols=100)
    for i, (waveforms, labels) in enumerate(pbar):
        waveforms = waveforms.to(device)
        labels = labels.to(device)
        
        try:
            logits = model(waveforms)
            loss = criterion(logits, labels)
            loss = loss / accumulation_steps  # Normalize
            
            loss.backward()
            
            if (i + 1) % accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
            
            total_loss += loss.item() * accumulation_steps
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            pbar.set_postfix({'loss': f'{loss.item() * accumulation_steps:.4f}'})
        except Exception as e:
            print(f"\nError in batch: {e}")
            continue
    
    # Always apply remaining gradients
    if len(all_preds) > 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()
    
    avg_loss = total_loss / max(len(dataloader), 1)
    accuracy = accuracy_score(all_labels, all_preds) if all_labels else 0.0
    
    return avg_loss, accuracy
```
‚úÖ model.train() –≤ –Ω–∞—á–∞–ª–µ
‚úÖ Loss –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è accumulation
‚úÖ Gradient clipping 1.0
‚úÖ –û—Å—Ç–∞—Ç–æ—á–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—Å–µ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è
‚úÖ Error handling —Å continue
‚úÖ Division by max(len, 1) –∑–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0

---

### **6. validate** ‚úÖ

```python
def validate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for waveforms, labels in tqdm(dataloader, desc="Validating", ncols=100):
            waveforms = waveforms.to(device)
            labels = labels.to(device)
            
            try:
                logits = model(waveforms)
                loss = criterion(logits, labels)
                
                total_loss += loss.item()
                preds = torch.argmax(logits, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
            except Exception as e:
                continue
    
    avg_loss = total_loss / max(len(dataloader), 1)
    accuracy = accuracy_score(all_labels, all_preds) if all_labels else 0.0
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='binary', zero_division=0
    ) if all_labels else (0, 0, 0, None)
    
    if all_labels:
        cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])
    else:
        cm = None
    
    return avg_loss, accuracy, precision, recall, f1, cm
```
‚úÖ model.eval() –≤ –Ω–∞—á–∞–ª–µ
‚úÖ torch.no_grad() –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
‚úÖ Confusion matrix —Å labels=[0, 1]
‚úÖ average='binary' –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
‚úÖ zero_division=0 –∑–∞—â–∏—Ç–∞
‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ all_labels –ø–µ—Ä–µ–¥ –º–µ—Ç—Ä–∏–∫–∞–º–∏

---

### **7. main() - Dataset loading** ‚úÖ

```python
# Load all samples
data_dir = Path(args.data_dir)
all_samples = []

normal_dir = data_dir / 'normal'
if normal_dir.exists():
    for wav_file in normal_dir.glob('*.wav'):
        all_samples.append((str(wav_file), 0))

patho_dir = data_dir / 'pathological'
if patho_dir.exists():
    for wav_file in patho_dir.glob('*.wav'):
        all_samples.append((str(wav_file), 1))

# Validate
if len(all_samples) == 0:
    raise ValueError("No audio files found!")

normal_count = sum(1 for _, l in all_samples if l == 0)
patho_count = sum(1 for _, l in all_samples if l == 1)

if normal_count == 0:
    raise ValueError("No normal samples found!")
if patho_count == 0:
    raise ValueError("No pathological samples found!")
```
‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Å—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤

---

### **8. main() - Dataset split** ‚úÖ

```python
indices = list(range(len(all_samples)))
random.Random(args.seed).shuffle(indices)

train_size = int(0.7 * len(indices))
val_size = int(0.15 * len(indices))
test_size = len(indices) - train_size - val_size

train_indices = indices[:train_size]
val_indices = indices[train_size:train_size + val_size]
test_indices = indices[train_size + val_size:]

train_samples = [all_samples[i] for i in train_indices]
val_samples = [all_samples[i] for i in val_indices]
test_samples = [all_samples[i] for i in test_indices]
```
‚úÖ –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–π shuffle —Å seed
‚úÖ 70/15/15 split
‚úÖ –ù–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π –º–µ–∂–¥—É train/val/test

---

### **9. main() - Dataset creation** ‚úÖ

```python
train_dataset = EnhancedVoiceDataset(
    train_samples, max_length=args.max_length, target_sr=args.target_sr, augment=True
)
val_dataset = EnhancedVoiceDataset(
    val_samples, max_length=args.max_length, target_sr=args.target_sr, augment=False
)
test_dataset = EnhancedVoiceDataset(
    test_samples, max_length=args.max_length, target_sr=args.target_sr, augment=False
)
```
‚úÖ –û—Ç–¥–µ–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (–Ω–µ shared reference)
‚úÖ target_sr –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è
‚úÖ augment=True —Ç–æ–ª—å–∫–æ –¥–ª—è train

---

### **10. main() - Training loop** ‚úÖ

```python
for epoch in range(1, args.epochs + 1):
    # Warmup BEFORE epoch
    if epoch <= args.warmup_epochs:
        warmup_lr = args.learning_rate * (epoch / args.warmup_epochs)
        for param_group in optimizer.param_groups:
            param_group['lr'] = warmup_lr
        log(f"Warmup LR: {warmup_lr:.2e}")
    
    # Train
    train_loss, train_acc = train_epoch(...)
    
    # Validate
    val_loss, val_acc, val_prec, val_rec, val_f1, cm = validate(...)
    
    # Update LR AFTER warmup
    if epoch > args.warmup_epochs:
        scheduler.step(val_loss)
    
    current_lr = optimizer.param_groups[0]['lr']
    
    # Save to history
    history['train_loss'].append(float(train_loss))
    # ... (–≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏)
    
    # Save best model
    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        torch.save({...}, best_path)
        patience_counter = 0
    else:
        patience_counter += 1
    
    # Save last model
    torch.save({...}, last_path)
    
    # Early stopping
    if patience_counter >= args.early_stop_patience:
        break
    
    # Checkpoint every 10 epochs
    if epoch % 10 == 0:
        torch.save({...}, checkpoint_path)
```
‚úÖ Warmup –ü–ï–†–ï–î —ç–ø–æ—Ö–æ–π
‚úÖ ReduceLROnPlateau –ü–û–°–õ–ï warmup
‚úÖ Early stopping –ø–æ F1
‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ best + last
‚úÖ Checkpoint –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö

---

### **11. main() - Final evaluation** ‚úÖ

```python
# Load best model
best_checkpoint = torch.load(output_dir / 'best.pt', map_location=device)
model.load_state_dict(best_checkpoint['model_state_dict'])

# Evaluate on validation
val_loss, val_acc, val_prec, val_rec, val_f1, cm = validate(model, val_loader, criterion, device)

# Evaluate on TEST
test_loss, test_acc, test_prec, test_rec, test_f1, test_cm = validate(model, test_loader, criterion, device)
```
‚úÖ map_location=device –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
‚úÖ –û—Ü–µ–Ω–∫–∞ –Ω–∞ val –∏ test
‚úÖ –ß–µ—Å—Ç–Ω–∞—è —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

---

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ #3:

### ‚úÖ **–£–°–ü–ï–®–ù–û –ü–†–û–ô–î–ï–ù–ê**

**–ù–∞–π–¥–µ–Ω–æ –æ—à–∏–±–æ–∫: 0**

**–í—Å–µ –∞—Å–ø–µ–∫—Ç—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π.**

---

## üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:

| –ê—Å–ø–µ–∫—Ç | –°—Ç–∞—Ç—É—Å |
|--------|--------|
| **–õ–æ–≥–∏–∫–∞** | ‚úÖ –ë–µ–∑—É–ø—Ä–µ—á–Ω–∞ |
| **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** | ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ |
| **–û–±—É—á–µ–Ω–∏–µ** | ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ |
| **–ú–µ—Ç—Ä–∏–∫–∏** | ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ |
| **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫** | ‚úÖ –ü–æ–ª–Ω–∞—è |
| **–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å** | ‚úÖ –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ |
| **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** | ‚úÖ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞ |
| **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** | ‚úÖ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ |

---

## ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ #3: –ü–†–û–ô–î–ï–ù–ê (2/3)

**–¢—Ä–µ–±—É–µ—Ç—Å—è –µ—â—ë 1 —É—Å–ø–µ—à–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞.**
